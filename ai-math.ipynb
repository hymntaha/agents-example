{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f58c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFLINE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657140c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "import gc\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca0e0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer,BitsAndBytesConfig,AutoModelForCausalLM, TrainingArguments\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "# Text embedding / Texxt Splitter for RAG \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# Adv RAG library \n",
    "\n",
    "# for evaluate  LLM \n",
    "import evaluate # require online \n",
    "# from deepeval.metrics import GEval\n",
    "# from deepeval.test_case import LLMTestCase\n",
    "# from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric , ContextualRelevancyMetric, ContextualRecallMetric\n",
    "import pytest\n",
    "# import trulens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c3f6752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20826fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearMemory():\n",
    "    for _ in range(5):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6461ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable/Disable Function\n",
    "FEW_SHOT_TEST= False#True\n",
    "USE_RAG = True#False#False #True#True\n",
    "USE_WANDB = False#True # for  LLM evalution and debug , track fine tuning performance\n",
    "USE_TRULENS = False # for LLM evalution\n",
    "USE_DEEPEVAL = False # for LLM evalution   (require openAI API key)\n",
    "USE_TRAIN =  True #True #False#True \n",
    "USE_INFER =  False # for submision prediction only , no test model \n",
    "if OFFLINE :\n",
    "    USE_WANDB = False # Wandb only support online  \n",
    "if device.type == \"cpu\": #requred GPU support for fine turning \n",
    "    USE_TRAIN= False\n",
    "\n",
    "if USE_WANDB:\n",
    "    # train report to  W&B tool\n",
    "    import wandb\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    reportTo= \"wandb\"\n",
    "    user_secrets = UserSecretsClient()\n",
    "    my_secret = user_secrets.get_secret(\"wandb_api_key\") \n",
    "    wandb.login(key=my_secret) # login \n",
    "else: \n",
    "    reportTo = \"none\"# None\n",
    "#     os.environ[\"WANDB_DISABLED\"] = True#“true”\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TRULENS:\n",
    "    from trulens_eval import Tru\n",
    "    tru = Tru()\n",
    "    tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c26aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reportTo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29622116",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == \"cuda\" and USE_TRAIN == True: #requred GPU support\n",
    "    # for LoRA fine tuning\n",
    "    from trl import SFTTrainer\n",
    "    from peft import LoraConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19ebffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmitFile = \"/kaggle/input/ai-mathematical-olympiad-prize/sample_submission.csv\"\n",
    "trainFile = \"/kaggle/input/ai-mathematical-olympiad-prize/train.csv\"\n",
    "testFile = \"/kaggle/input/ai-mathematical-olympiad-prize/test.csv\"\n",
    "mathQSATrainFile = \"/kaggle/input/math-qsa-dataset/train.csv\"\n",
    "mathQSATestFile = \"/kaggle/input/math-qsa-dataset/test.csv\"\n",
    "gsm8kTrainFile = \"/kaggle/input/gsm8k-grade-school-math-8k-dataset-for-llm/gsm8k/main/train-00000-of-00001.parquet\"\n",
    "gsm8kTestFile = \"/kaggle/input/gsm8k-grade-school-math-8k-dataset-for-llm/gsm8k/main/test-00000-of-00001.parquet\"\n",
    "mathQATrainFile = \"/kaggle/input/math-qa-for-aqua-rat-dataset/MathQA/train.json\"\n",
    "mathQATestFile = \"/kaggle/input/math-qa-for-aqua-rat-dataset/MathQA/test.json\"\n",
    "orcaMath200kFile = \"/kaggle/input/microsoftorca-math-word-problems-200k/orca-math-word-problems-200k/data/train-00000-of-00001.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce5841",
   "metadata": {},
   "outputs": [],
   "source": [
    "clearMemory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.read_csv(trainFile)\n",
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f44fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d9585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.iloc[7][\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedf9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.read_csv(testFile)\n",
    "testDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bc5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainQSADF = pd.read_csv(mathQSATrainFile)\n",
    "trainQSADF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454afd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainQSADF[\"problem\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainQSADF.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5835c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainQADF= pd.read_json(mathQATrainFile)\n",
    "trainQADF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467776f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainQADF[\"options\"][5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGSM8KDF =pd.read_parquet(gsm8kTrainFile)\n",
    "trainGSM8KDF.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGSM8KDF.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainGSM8KDF.iloc[0][\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae25949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testGSM8KDF = pd.read_parquet(gsm8kTestFile)\n",
    "testGSM8KDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c19547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testGSM8KDF[\"answer\"][12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainOracMath200kDF = pd.read_parquet(orcaMath200kFile)\n",
    "trainOracMath200kDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6999b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainOracMath200kDF[\"answer\"].iloc[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220fc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF[\"problem\"][9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfd3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning data set\n",
    "# trainDF[\"problem\"] = trainDF[\"problem\"].str.replace(\"$\", '')\n",
    "# trainDF[\"problem\"] = trainDF[\"problem\"].str.replace(\"\\\\vert\", '|')\n",
    "# trainDF[\"problem\"] = trainDF[\"problem\"].str.replace(\"\\\\left\", '')\n",
    "# trainDF[\"problem\"] = trainDF[\"problem\"].str.replace(\"\\\\right\", '')\n",
    "# trainDF[\"problem\"] = trainDF[\"problem\"].str.replace(\"\\\\mathbb\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa32dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainDF[\"problem\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e25de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainDF[\"answer\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model \n",
    "USE_LLAMA3 = False # for GPU version\n",
    "modelName1 = \"/kaggle/input/gemma/transformers/2b-it/3\"\n",
    "modelName2 =  \"/kaggle/input/gemma/transformers/7b-it/3\" # careful memory usage , will out of Memory both CPU or GPU\n",
    "modelName3 =  \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\" \n",
    "do_sample= True \n",
    "top_p=0.95 \n",
    "top_k= 2\n",
    "temperature=0.2#0.7 \n",
    "num_beams = 3\n",
    "max_length= 512\n",
    "\n",
    "# Quantized Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant=True # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b2ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if device.type == \"cuda\": # use 7b model gain Math performance\n",
    "    if USE_LLAMA3: \n",
    "        modelSel = modelName3\n",
    "        llmModel = \"llama3_8b\"\n",
    "    else: \n",
    "        modelSel = modelName2\n",
    "        llmModel = \"gemma_7b\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelSel , device_map=\"auto\", quantization_config= bnb_config)   # intial with GPU quantized\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer\n",
    "else: \n",
    "    modelSel = modelName1\n",
    "    llmModel = \"gemma_2b\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(modelSel , device_map=\"auto\")   # intial \n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbcd743",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c34983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateResponse(query, maxOutToken=256):\n",
    "    \"\"\"\n",
    "     Direct send message to gemini, get response\n",
    "    \"\"\"\n",
    "    inputIds = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "    response = model.generate(**inputIds , \n",
    "                              do_sample =True,\n",
    "                              top_p = 0.95,\n",
    "                              top_k= 2,\n",
    "                              temperature= 0.2, #0.7,#0.3,#0.7,\n",
    "#                               max_length=maxOutToken,\n",
    "                              max_new_tokens= maxOutToken,\n",
    "                             )\n",
    "#     return tokenizer.decode(response[0])\n",
    "    return tokenizer.decode(response[0][len(inputIds[\"input_ids\"]):], skip_special_tokens = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9a8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateReponseInst(promptTemp, query, maxOutToken=256):\n",
    "    \"\"\"\n",
    "    Insert prompt template instruction with message\n",
    "    \"\"\"\n",
    "#     prompt = f\"\"\"{promptTemp}\\nQuestion: {query}\\nAnswer: \n",
    "#     \"\"\"\n",
    "    prompt = f\"\"\"{promptTemp}\\nQuestion: {query}\\n### Instruction: Given Answer in JSON format with key 'answer' and 'explanation' ### \n",
    "    \"\"\"\n",
    "    inputIds = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    response = model.generate(**inputIds,\n",
    "                              do_sample =True,\n",
    "                              top_p = 0.95,\n",
    "                              top_k= 2,\n",
    "                              temperature= 0.2, #0.7, # 0.3,#0.7,\n",
    "#                               max_length=maxOutToken,\n",
    "                              max_new_tokens= maxOutToken,)\n",
    "#     return tokenizer.decode(response[0]) # reutrn \n",
    "    return tokenizer.decode(response[0][len(inputIds[\"input_ids\"]):], skip_special_tokens = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04591c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateReponseRAG(promptTemp, ragContext, query, maxOutToken=256):\n",
    "    \"\"\"\n",
    "    Use Insert prompt insturction, RAG retrieve with query \n",
    "    \"\"\"\n",
    "    info = \"\\n\".join(ragContext)\n",
    "    prompt = f\"\"\"\n",
    "    {promptTemp}\\n\n",
    "    Question: {query}\\n\n",
    "    Information: {info}\\n\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    inputIds = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    response = model.generate(**inputIds, \n",
    "                              do_sample =True,\n",
    "                              top_p = 0.95,\n",
    "                              top_k= 2,\n",
    "                              temperature= 0.2, #0.7, #0.3, #0.7,\n",
    "#                               max_length=maxOutToken,\n",
    "                              max_new_tokens= maxOutToken,)\n",
    "    return tokenizer.decode(response[0][len(inputIds[\"input_ids\"]):], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08252768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8cce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json.decoder import JSONDecodeError\n",
    "def isInteger(text):\n",
    "    try:\n",
    "        if int(text) >= 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def llmJSONParser1(txt):\n",
    "#     print(txt)\n",
    "    try:\n",
    "        txt = txt.replace(\"<eos>\", \"\")\n",
    "        subTxt = txt.split(\"```\")\n",
    "        subTxt[1] = subTxt[1].replace(\"json\", \"\")\n",
    "        subTxt[1] = subTxt[1].replace(\"<eos>\", \"\")\n",
    "#             print(subTxt[1])\n",
    "        jsonTxt = json.loads(subTxt[1])\n",
    "    except  JSONDecodeError as e:\n",
    "        print(\"Error LLM JSON parser\", e)\n",
    "        return None\n",
    "    except :\n",
    "        print(f\"\"\"Error LLM JSON parser input txt {txt}\"\"\" )\n",
    "        return None\n",
    "    return jsonTxt\n",
    "\n",
    "\n",
    "def llmJSONParser2(txt):\n",
    "#     print(txt)\n",
    "    try:\n",
    "        subText = txt.split(\"{\")\n",
    "        start = txt.find(\"{\")\n",
    "        end = txt.find(\"}\")\n",
    "        print(f\"Start loc: {start}, End loc: {end}\")\n",
    "        subString = txt[start:end+1]\n",
    "        print(subString)\n",
    "        jsonTxt = json.loads(subString)\n",
    "    except  JSONDecodeError as e:\n",
    "        print(\"Error LLM JSON parser\", e)\n",
    "        return None\n",
    "    except :\n",
    "        print(f\"\"\"Error LLM JSON parser input txt {txt}\"\"\" )\n",
    "        return None\n",
    "    return jsonTxt\n",
    "    \n",
    "def llmJSONParser3(txt):\n",
    "    '''\n",
    "    Manual JSON answer parser without , json library \n",
    "    '''\n",
    "    try:\n",
    "        subText = txt.split(\"{\") # split several {} in list\n",
    "        for txtSeg in subText: # loop in list to find answer\n",
    "            end = txtSeg.find(\"}\") # find end position in text segment\n",
    "            sub = txtSeg[:end] #subsring with {} context\n",
    "            temp = sub.replace(\"*\", \"\") # remove * symbol\n",
    "            temp = temp.replace(\"\\\"\", \"\") # reomve \\\" symbol\n",
    "            temp = temp.lower() # convert to lower case\n",
    "            answerloc = temp.find(\"answer:\") # find key word \"answer\" position\n",
    "            if answerloc != -1:\n",
    "                print(f\"find answer location : {answerloc}\")\n",
    "                newTxt = temp[answerloc:] # substring start answer\n",
    "#                 print(\"Temp: \", temp)\n",
    "                subTxt = newTxt.split(\"\\n\")\n",
    "                #       print(subTxt)\n",
    "                rel =subTxt[0][len(\"answer:\"):].strip() # get answer value with remove space\n",
    "                rel= rel.replace(',', '') # remove , symbol\n",
    "                print(rel)\n",
    "                if isInteger(rel):\n",
    "                    return rel\n",
    "                else:\n",
    "                    continue # not find the value\n",
    "#                 print(rel)\n",
    "                \n",
    "                \n",
    "        return None # can't find answer\n",
    "\n",
    "    except  JSONDecodeError as e:\n",
    "        print(\"Error LLM JSON parser\", e)\n",
    "        return None\n",
    "    except :\n",
    "        print(f\"\"\"Error LLM JSON parser input txt {txt}\"\"\" )\n",
    "        return None\n",
    "    return jsonTxt\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
